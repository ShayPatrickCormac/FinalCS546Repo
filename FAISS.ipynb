{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_paths = []\n",
    "def preprocess_documents(directory):\n",
    "    combined_text = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if filepath.endswith(\".txt\"):\n",
    "            with open(filepath, 'r') as file:\n",
    "                combined_text.append(file.read())\n",
    "                document_paths.append(filepath)\n",
    "    return combined_text\n",
    "\n",
    "directory = \"./downloaded_pages/Block/Normal_Blocks\"\n",
    "documents = preprocess_documents(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "document_vectors = vectorizer.fit_transform(documents)\n",
    "document_vectors_dense = document_vectors.toarray().astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = document_vectors_dense.shape[1]\n",
    "\n",
    "# Creating a FAISS index\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(document_vectors_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_in_index(query, k=5):\n",
    "    query_vector = vectorizer.transform([query]).toarray().astype('float32')\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    results = []\n",
    "    for idx in indices[0]: \n",
    "        document_title_or_excerpt = get_document_title_or_excerpt(idx)\n",
    "        results.append((idx, document_title_or_excerpt))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_document_title_or_excerpt(index):\n",
    "    return f\"Document at index: {index}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Index: 50, Document Path: ./downloaded_pages/Block/Normal_Blocks\\Cake.txt\n",
      "Document Index: 42, Document Path: ./downloaded_pages/Block/Normal_Blocks\\Blue_Orchid.txt\n",
      "Document Index: 103, Document Path: ./downloaded_pages/Block/Normal_Blocks\\Dandelion.txt\n",
      "Document Index: 56, Document Path: ./downloaded_pages/Block/Normal_Blocks\\Carrot.txt\n",
      "Document Index: 146, Document Path: ./downloaded_pages/Block/Normal_Blocks\\Glow_Berries.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"hunger\"\n",
    "search_results = search_in_index(query)\n",
    "\n",
    "for result in search_results:\n",
    "    print(f\"Document Index: {result[0]}, Document Path: {document_paths[result[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "    \n",
    "def split_into_chunks(document, max_length=500):\n",
    "    tokenized_text = tokenizer.tokenize(document)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for token in tokenized_text:\n",
    "        current_chunk.append(token)\n",
    "        current_length += 1\n",
    "\n",
    "        if current_length == max_length:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            print(len(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def answer_question(question, answer_text):\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    input_ids = torch.tensor([input_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokens[answer_start:answer_end])\n",
    "    print(answer)\n",
    "\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_best_answer(question, document_paths):\n",
    "    best_answer = \"\"\n",
    "    highest_score = -float('inf')\n",
    "\n",
    "    for path in document_paths:\n",
    "        document = read_document(path)\n",
    "        chunks = split_into_chunks(document)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            answer = answer_question(question, chunk)\n",
    "            # TODO: Add logic to evaluate confidence\n",
    "\n",
    "            print(answer)\n",
    "\n",
    "    return best_answer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nquestion = \"How can a wet sponge be turned back into a normal sponge?\"\\nsearch_results = search_in_index(question)\\ntop_document_paths = []\\n\\nfor result in search_results:\\n    top_document_paths.append(document_paths[result[0]])\\n    print(document_paths[result[0]])\\n\\nanswer = get_best_answer(question, top_document_paths)\\nprint(\"Answer:\", answer)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "question = \"How can a wet sponge be turned back into a normal sponge?\"\n",
    "search_results = search_in_index(question)\n",
    "top_document_paths = []\n",
    "\n",
    "for result in search_results:\n",
    "    top_document_paths.append(document_paths[result[0]])\n",
    "    print(document_paths[result[0]])\n",
    "\n",
    "answer = get_best_answer(question, top_document_paths)\n",
    "print(\"Answer:\", answer)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class DocumentReader:\n",
    "    def __init__(self):\n",
    "        #self.READER_PATH = pretrained_model_name_or_path\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "        self.model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "        self.max_len = self.model.config.max_position_embeddings\n",
    "        self.chunked = False\n",
    "\n",
    "    def tokenize(self, question, text):\n",
    "        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "        if len(self.input_ids) > self.max_len:\n",
    "            self.inputs = self.chunkify()\n",
    "            self.chunked = True\n",
    "\n",
    "    def chunkify(self):\n",
    "        qmask = self.inputs['token_type_ids'].lt(1)\n",
    "        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n",
    "        chunk_size = self.max_len - qt.size()[0] - 1\n",
    "\n",
    "        chunked_input = OrderedDict()\n",
    "        for k,v in self.inputs.items():\n",
    "            q = torch.masked_select(v, qmask)\n",
    "            c = torch.masked_select(v, ~qmask)\n",
    "            chunks = torch.split(c, chunk_size)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if i not in chunked_input:\n",
    "                    chunked_input[i] = {}\n",
    "\n",
    "                thing = torch.cat((q, chunk))\n",
    "                if i != len(chunks)-1:\n",
    "                    if k == 'input_ids':\n",
    "                        thing = torch.cat((thing, torch.tensor([102])))\n",
    "                    else:\n",
    "                        thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n",
    "        return chunked_input\n",
    " \n",
    "    def get_answer(self):\n",
    "        if self.chunked:\n",
    "            answer = ''\n",
    "            for k, chunk in self.inputs.items():\n",
    "                outputs = self.model(**chunk)\n",
    "                answer_start_scores = outputs.start_logits\n",
    "                answer_end_scores = outputs.end_logits\n",
    "                answer_start = torch.argmax(answer_start_scores)\n",
    "                \n",
    "                answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n",
    "                if ans != '[CLS]':\n",
    "                    answer += ans + \" \"\n",
    "            return answer\n",
    "        else:\n",
    "            outputs = self.model(**self.inputs)\n",
    "            answer_start_scores = outputs.start_logits\n",
    "            answer_end_scores = outputs.end_logits\n",
    "\n",
    "            answer_start = torch.argmax(answer_start_scores)\n",
    "            answer_end = torch.argmax(answer_end_scores) + 1\n",
    "        \n",
    "            return self.convert_ids_to_string(self.inputs['input_ids'][0][\n",
    "                                              answer_start:answer_end])\n",
    "\n",
    "    def convert_ids_to_string(self, input_ids):\n",
    "        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./downloaded_pages/Block/Normal_Blocks\\Budding_Amethyst.txt\n",
      "./downloaded_pages/Block/Normal_Blocks\\Amethyst_Cluster.txt\n",
      "./downloaded_pages/Block/Normal_Blocks\\Block_of_Amethyst.txt\n",
      "./downloaded_pages/Block/Normal_Blocks\\Tinted_Glass.txt\n",
      "./downloaded_pages/Block/Normal_Blocks\\Calcite.txt\n",
      "Answer: via the creative inventory or with commands\n"
     ]
    }
   ],
   "source": [
    "reader = DocumentReader()\n",
    "question = \"How can players obtain budding amethyst blocks in the game?\"\n",
    "search_results = search_in_index(question)\n",
    "top_document_paths = []\n",
    "\n",
    "for result in search_results:\n",
    "    top_document_paths.append(document_paths[result[0]])\n",
    "    print(document_paths[result[0]])\n",
    "\n",
    "document = read_document(top_document_paths[0])\n",
    "reader.tokenize(question, document)\n",
    "print(f\"Answer: {reader.get_answer()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    reader = DocumentReader()\n",
    "    search_results = search_in_index(question)\n",
    "    top_document_paths = []\n",
    "    for result in search_results:\n",
    "        top_document_paths.append(document_paths[result[0]])\n",
    "        #print(document_paths[result[0]])\n",
    "    document = read_document(top_document_paths[0])\n",
    "    reader.tokenize(question, document)\n",
    "    return reader.get_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('./part_scraped_final_minecraft_dataset(1).json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "qa_data = []\n",
    "for entry in data['data']:\n",
    "    title = entry['title']\n",
    "    for paragraph in entry['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            id = qa['id']\n",
    "            for answer in qa['answers']:\n",
    "                answer_text = answer['text']\n",
    "                answer_start = answer['answer_start']\n",
    "                qa_data.append({'title': title, 'context': context, 'question': question, 'id': id, 'answer_text': answer_text, 'answer_start': answer_start})\n",
    "\n",
    "df = pd.DataFrame(qa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sponge</td>\n",
       "      <td>Overview\\n    A sponge is a block that can be ...</td>\n",
       "      <td>How can a wet sponge be turned back into a nor...</td>\n",
       "      <td>q1</td>\n",
       "      <td>A wet sponge can be dried in a furnace, making...</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sponge</td>\n",
       "      <td>Overview\\n    A sponge is a block that can be ...</td>\n",
       "      <td>Where can sponges be found in Minecraft?</td>\n",
       "      <td>q2</td>\n",
       "      <td>Sponges can only be found in ocean monuments.</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sponge</td>\n",
       "      <td>Overview\\n    A sponge is a block that can be ...</td>\n",
       "      <td>What is the maximum distance a sponge can abso...</td>\n",
       "      <td>q3</td>\n",
       "      <td>A sponge absorbs both flowing and source block...</td>\n",
       "      <td>1935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leaves</td>\n",
       "      <td>Overview\\n    Leaves are natural blocks that g...</td>\n",
       "      <td>What are the default tools for breaking leaves...</td>\n",
       "      <td>q1</td>\n",
       "      <td>Hoes are the default tools for breaking leaves.</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leaves</td>\n",
       "      <td>Overview\\n    Leaves are natural blocks that g...</td>\n",
       "      <td>Which blocks are affected by the biome dyeing ...</td>\n",
       "      <td>q2</td>\n",
       "      <td>Leaves are affected by the biome dyeing algori...</td>\n",
       "      <td>5351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>End_Portal_(block)</td>\n",
       "      <td>Overview\\n    The end portal block is a block ...</td>\n",
       "      <td>How can end portal blocks be placed in Java Ed...</td>\n",
       "      <td>q2</td>\n",
       "      <td>It can be placed only by using block placement...</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>End_Portal_(block)</td>\n",
       "      <td>Overview\\n    The end portal block is a block ...</td>\n",
       "      <td>What happens when entities travel to the Overw...</td>\n",
       "      <td>q3</td>\n",
       "      <td>Players traveling to the Overworld appear at t...</td>\n",
       "      <td>829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>Bubble_Column</td>\n",
       "      <td>Overview\\n    A bubble column is a non-solid b...</td>\n",
       "      <td>What are the two ways in which a bubble column...</td>\n",
       "      <td>q1</td>\n",
       "      <td>A bubble column is a non-solid block generated...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>Bubble_Column</td>\n",
       "      <td>Overview\\n    A bubble column is a non-solid b...</td>\n",
       "      <td>How can players and mobs make use of bubble co...</td>\n",
       "      <td>q2</td>\n",
       "      <td>Players and air-breathing mobs can enter a bub...</td>\n",
       "      <td>1180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>Bubble_Column</td>\n",
       "      <td>Overview\\n    A bubble column is a non-solid b...</td>\n",
       "      <td>What is the effect of whirlpool bubble columns...</td>\n",
       "      <td>q3</td>\n",
       "      <td>A boat ridden over a whirlpool shakes and even...</td>\n",
       "      <td>1805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1087 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title                                            context  \\\n",
       "0                 Sponge  Overview\\n    A sponge is a block that can be ...   \n",
       "1                 Sponge  Overview\\n    A sponge is a block that can be ...   \n",
       "2                 Sponge  Overview\\n    A sponge is a block that can be ...   \n",
       "3                 Leaves  Overview\\n    Leaves are natural blocks that g...   \n",
       "4                 Leaves  Overview\\n    Leaves are natural blocks that g...   \n",
       "...                  ...                                                ...   \n",
       "1082  End_Portal_(block)  Overview\\n    The end portal block is a block ...   \n",
       "1083  End_Portal_(block)  Overview\\n    The end portal block is a block ...   \n",
       "1084       Bubble_Column  Overview\\n    A bubble column is a non-solid b...   \n",
       "1085       Bubble_Column  Overview\\n    A bubble column is a non-solid b...   \n",
       "1086       Bubble_Column  Overview\\n    A bubble column is a non-solid b...   \n",
       "\n",
       "                                               question  id  \\\n",
       "0     How can a wet sponge be turned back into a nor...  q1   \n",
       "1              Where can sponges be found in Minecraft?  q2   \n",
       "2     What is the maximum distance a sponge can abso...  q3   \n",
       "3     What are the default tools for breaking leaves...  q1   \n",
       "4     Which blocks are affected by the biome dyeing ...  q2   \n",
       "...                                                 ...  ..   \n",
       "1082  How can end portal blocks be placed in Java Ed...  q2   \n",
       "1083  What happens when entities travel to the Overw...  q3   \n",
       "1084  What are the two ways in which a bubble column...  q1   \n",
       "1085  How can players and mobs make use of bubble co...  q2   \n",
       "1086  What is the effect of whirlpool bubble columns...  q3   \n",
       "\n",
       "                                            answer_text  answer_start  \n",
       "0     A wet sponge can be dried in a furnace, making...          1067  \n",
       "1         Sponges can only be found in ocean monuments.           250  \n",
       "2     A sponge absorbs both flowing and source block...          1935  \n",
       "3       Hoes are the default tools for breaking leaves.           114  \n",
       "4     Leaves are affected by the biome dyeing algori...          5351  \n",
       "...                                                 ...           ...  \n",
       "1082  It can be placed only by using block placement...           270  \n",
       "1083  Players traveling to the Overworld appear at t...           829  \n",
       "1084  A bubble column is a non-solid block generated...            31  \n",
       "1085  Players and air-breathing mobs can enter a bub...          1180  \n",
       "1086  A boat ridden over a whirlpool shakes and even...          1805  \n",
       "\n",
       "[1087 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2190 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (606 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2024 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2195 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2024 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (603 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1635 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1265 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1266 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1262 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (831 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (835 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (831 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (838 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (841 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (840 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2404 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2414 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2409 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1272 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1860 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1140 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1140 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1144 > 512). Running this sequence through the model will result in indexing errors\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "questions = df['question'].head(50).to_list()\n",
    "contexts = df['context'].head(50).to_list()\n",
    "candidates = []\n",
    "for i in range (50):\n",
    "    question = questions[i]\n",
    "    context = contexts[i]\n",
    "    predicted_answer = ask_question(question)\n",
    "    # print(\"Q:\", question)\n",
    "    # print(\"A:\", predicted_answer)\n",
    "    candidates.append(predicted_answer)\n",
    "\n",
    "references = df['answer_text'].head(50).to_list()\n",
    "P, R, F1 = score(candidates, references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8413)\n",
      "tensor(0.8727)\n",
      "tensor(0.8406)\n"
     ]
    }
   ],
   "source": [
    "print(P.mean())\n",
    "print(R.mean())\n",
    "print(F1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using bert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class DocumentReader:\n",
    "    def __init__(self):\n",
    "        #self.READER_PATH = pretrained_model_name_or_path\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "        self.max_len = self.model.config.max_position_embeddings\n",
    "        self.chunked = False\n",
    "\n",
    "    def tokenize(self, question, text):\n",
    "        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n",
    "        if len(self.input_ids) > self.max_len:\n",
    "            self.inputs = self.chunkify()\n",
    "            self.chunked = True\n",
    "\n",
    "    def chunkify(self):\n",
    "        qmask = self.inputs['token_type_ids'].lt(1)\n",
    "        qt = torch.masked_select(self.inputs['input_ids'], qmask)\n",
    "        chunk_size = self.max_len - qt.size()[0] - 1\n",
    "\n",
    "        chunked_input = OrderedDict()\n",
    "        for k,v in self.inputs.items():\n",
    "            q = torch.masked_select(v, qmask)\n",
    "            c = torch.masked_select(v, ~qmask)\n",
    "            chunks = torch.split(c, chunk_size)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if i not in chunked_input:\n",
    "                    chunked_input[i] = {}\n",
    "\n",
    "                thing = torch.cat((q, chunk))\n",
    "                if i != len(chunks)-1:\n",
    "                    if k == 'input_ids':\n",
    "                        thing = torch.cat((thing, torch.tensor([102])))\n",
    "                    else:\n",
    "                        thing = torch.cat((thing, torch.tensor([1])))\n",
    "\n",
    "                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n",
    "        return chunked_input\n",
    " \n",
    "    def get_answer(self):\n",
    "        if self.chunked:\n",
    "            answer = ''\n",
    "            for k, chunk in self.inputs.items():\n",
    "                outputs = self.model(**chunk)\n",
    "                answer_start_scores = outputs.start_logits\n",
    "                answer_end_scores = outputs.end_logits\n",
    "                answer_start = torch.argmax(answer_start_scores)\n",
    "                \n",
    "                answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n",
    "                if ans != '[CLS]':\n",
    "                    answer += ans + \" \"\n",
    "            return answer\n",
    "        else:\n",
    "            outputs = self.model(**self.inputs)\n",
    "            answer_start_scores = outputs.start_logits\n",
    "            answer_end_scores = outputs.end_logits\n",
    "\n",
    "            answer_start = torch.argmax(answer_start_scores)\n",
    "            answer_end = torch.argmax(answer_end_scores) + 1\n",
    "        \n",
    "            return self.convert_ids_to_string(self.inputs['input_ids'][0][\n",
    "                                              answer_start:answer_end])\n",
    "\n",
    "    def convert_ids_to_string(self, input_ids):\n",
    "        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./downloaded_pages/Block/Normal_Blocks\\Budding_Amethyst.txt\n",
      "./downloaded_pages/Block/Normal_Blocks\\Amethyst_Cluster.txt\n",
      "./downloaded_pages/Block/Normal_Blocks\\Block_of_Amethyst.txt\n",
      "./downloaded_pages/Block/Normal_Blocks\\Tinted_Glass.txt\n",
      "./downloaded_pages/Block/Normal_Blocks\\Calcite.txt\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "reader = DocumentReader()\n",
    "question = \"How can players obtain budding amethyst blocks in the game?\"\n",
    "search_results = search_in_index(question)\n",
    "top_document_paths = []\n",
    "\n",
    "for result in search_results:\n",
    "    top_document_paths.append(document_paths[result[0]])\n",
    "    print(document_paths[result[0]])\n",
    "\n",
    "document = read_document(top_document_paths[0])\n",
    "reader.tokenize(question, document)\n",
    "print(f\"Answer: {reader.get_answer()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    reader = DocumentReader()\n",
    "    search_results = search_in_index(question)\n",
    "    top_document_paths = []\n",
    "    for result in search_results:\n",
    "        top_document_paths.append(document_paths[result[0]])\n",
    "        #print(document_paths[result[0]])\n",
    "    document = read_document(top_document_paths[0])\n",
    "    reader.tokenize(question, document)\n",
    "    return reader.get_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('./part_scraped_final_minecraft_dataset(1).json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "qa_data = []\n",
    "for entry in data['data']:\n",
    "    title = entry['title']\n",
    "    for paragraph in entry['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            id = qa['id']\n",
    "            for answer in qa['answers']:\n",
    "                answer_text = answer['text']\n",
    "                answer_start = answer['answer_start']\n",
    "                qa_data.append({'title': title, 'context': context, 'question': question, 'id': id, 'answer_text': answer_text, 'answer_start': answer_start})\n",
    "\n",
    "df = pd.DataFrame(qa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "questions = df['question'].head(50).to_list()\n",
    "contexts = df['context'].head(50).to_list()\n",
    "candidates = []\n",
    "for i in range (50):\n",
    "    question = questions[i]\n",
    "    context = contexts[i]\n",
    "    predicted_answer = ask_question(question)\n",
    "    # print(\"Q:\", question)\n",
    "    # print(\"A:\", predicted_answer)\n",
    "    candidates.append(predicted_answer)\n",
    "\n",
    "references = df['answer_text'].head(50).to_list()\n",
    "P, R, F1 = score(candidates, references, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5359)\n",
      "tensor(0.8355)\n",
      "tensor(0.5642)\n"
     ]
    }
   ],
   "source": [
    "print(P.mean())\n",
    "print(R.mean())\n",
    "print(F1.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
