{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import time\n",
    "import urllib.robotparser\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_WIKI_URL = \"https://minecraft.wiki\"\n",
    "#SEED_URL = f\"{BASE_URL}/Minecraft_Wiki\"\n",
    "SAVE_PATH = \"downloaded_pages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help Functions\n",
    "def write_content(file, section_title, subsection_title, content):\n",
    "    if section_title:\n",
    "        file.write(f\"{section_title}\\n\")\n",
    "    if subsection_title:\n",
    "        file.write(f\"  {subsection_title}\\n\")\n",
    "    if content:\n",
    "        file.write(f\"    {' '.join(content)}\\n\\n\")\n",
    "\n",
    "def write_brew_recipe(cell):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    input_span = cell.select_one('.mcui-input')\n",
    "    if input_span:\n",
    "        input_items = input_span.select('.invslot-item-image a')\n",
    "        for item in input_items:\n",
    "            item_title = item.get('title')\n",
    "            if item_title:\n",
    "                inputs.append(item_title)\n",
    "        input_items = input_span.select('.invslot-item invslot-item-image animated-active a')\n",
    "        for item in input_items:\n",
    "            item_title = item.get('title')\n",
    "            if item_title:\n",
    "                inputs.append(item_title)\n",
    "    output_container = cell.select_one('.mcui-output')\n",
    "    if output_container:\n",
    "        output_items = output_container.select('.invslot-item-image a')\n",
    "        for item in output_items:\n",
    "            item_title = item.get('title')\n",
    "            if item_title:\n",
    "                outputs.append(item_title)\n",
    "    recipe_parts = []\n",
    "    if inputs:\n",
    "        recipe_parts.append(\"input:\" + \"; \".join(inputs))\n",
    "    if outputs:\n",
    "        recipe_parts.append(\"output:\" + \"; \".join(outputs))\n",
    "    return ' | '.join(recipe_parts)\n",
    "\n",
    "def write_with_sprite(cell):\n",
    "    img_tags = cell.find_all('img', class_='pixel-image')\n",
    "    sprite_names = []\n",
    "    for img_tag in img_tags:\n",
    "        alt_text = img_tag.get('alt', '')\n",
    "        sprite_name = alt_text.split(' ')[1]\n",
    "        sprite_name = sprite_name.replace('.png', '').replace('-', ' ').replace(':','')\n",
    "        sprite_names.append(sprite_name)\n",
    "        sprite_names_text = '; '.join(sprite_names)\n",
    "        additional_text = ' '.join(cell.stripped_strings)\n",
    "        combined_text = f\"{sprite_names_text} {additional_text}\".strip()\n",
    "    return combined_text\n",
    "\n",
    "def write_smithing_recipe(cell):\n",
    "    inputs = []\n",
    "    items = cell.select('.invslot-item-image a')\n",
    "    if items:\n",
    "        for item in items:\n",
    "            item_title = item.get('title')\n",
    "            if item_title:\n",
    "                inputs.append(item_title)\n",
    "    output = inputs[-1] \n",
    "    inputs = inputs[:-1]\n",
    "    recipe_parts = []\n",
    "    recipe_parts.append(\"input:\" + \"; \".join(inputs))\n",
    "    recipe_parts.append(\"output:\" + output)\n",
    "    recipe_str = ' | '.join(recipe_parts)\n",
    "    return recipe_str\n",
    "\n",
    "\n",
    "def write_crafting_recipe(cell):\n",
    "    mcui_input = cell.find('span', class_='mcui-input')\n",
    "    if mcui_input:\n",
    "        inv_slots = mcui_input.find_all('span', class_='invslot')\n",
    "        recipe_pattern = []\n",
    "        for slot in inv_slots:\n",
    "            item = slot.find('span', class_='invslot-item')\n",
    "            if item:\n",
    "                title = item.a.get('title') if item.a else ''\n",
    "                recipe_pattern.append(title)\n",
    "            else:\n",
    "                recipe_pattern.append(None)\n",
    "        three_by_three_recipe = [recipe_pattern[i:i+3] for i in range(0, len(recipe_pattern), 3)]\n",
    "        return str(three_by_three_recipe)\n",
    "    return \"\"\n",
    "    \n",
    "\n",
    "def write_table(table, file):\n",
    "    rows = table.find_all('tr')\n",
    "    csv_writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    for row in rows:\n",
    "        row_text = []\n",
    "        for cell in row.find_all(['td','th']):\n",
    "            if cell.find('span', class_='mcui mcui-Brewing_Stand pixel-image'):\n",
    "                cell_text = write_brew_recipe(cell)\n",
    "            elif cell.find('span', class_='mcui mcui-Smithing_Table pixel-image'):\n",
    "                cell_text = write_smithing_recipe(cell)\n",
    "            elif cell.find('span', class_='mcui mcui-Crafting_Table pixel-image'):\n",
    "                cell_text = write_crafting_recipe(cell)\n",
    "            elif cell.get_text(separator=\" \", strip=True) == '' and cell.find('span', class_='sprite-file'):\n",
    "                cell_text = write_with_sprite(cell)\n",
    "            else:\n",
    "                cell_text = cell.get_text(separator=\" \", strip=True)\n",
    "            row_text.append(cell_text)\n",
    "        file.write('    ')\n",
    "        csv_writer.writerow(row_text)\n",
    "    file.write('\\n')\n",
    "\n",
    "\n",
    "def crawl_page(url, save_path, pagename):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    with open(save_path + pagename + \".txt\", 'w', encoding='utf-8') as file:\n",
    "        main_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "        section_title = \"Overview\"\n",
    "        subsection_title = \"\"\n",
    "        content = []\n",
    "        \n",
    "        for element in main_div.find_all(['h2', 'h3', 'dl', 'p', 'table', 'ul'], recursive=False):\n",
    "            # print(f'Processing {element.name} tag')\n",
    "            if element.name == 'h2':\n",
    "                # Write previous section\n",
    "                write_content(file, section_title, subsection_title, content)\n",
    "                content = []\n",
    "                section_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "                if section_title == \"History\" or section_title == \"Sounds\" or section_title == \"Achievements\" or section_title == \"Advancements\":\n",
    "                    break\n",
    "                subsection_title = \"\"\n",
    "            elif element.name == 'h3':\n",
    "                # Write previous subsection + content\n",
    "                write_content(file, section_title, subsection_title, content)\n",
    "                content = []\n",
    "                section_title = \"\"\n",
    "                subsection_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "            elif element.name == 'p' or element.name == 'dl':\n",
    "                content.append(element.text.strip())\n",
    "            elif element.name == 'ul':\n",
    "                temp = ', '.join([li.get_text() for li in element.find_all('li')])\n",
    "                content.append(temp)\n",
    "            elif element.name == 'table':\n",
    "                write_content(file, section_title, subsection_title, content)\n",
    "                write_table(element, file)\n",
    "                section_title = \"\"\n",
    "                subsection_title = \"\"\n",
    "                content = []\n",
    "\n",
    "        if content:\n",
    "            write_content(file, section_title, subsection_title, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://minecraft.wiki/w/Block\n",
      "https://minecraft.wiki/w/Tulip\n",
      "https://minecraft.wiki/w/Banner\n"
     ]
    }
   ],
   "source": [
    "# Test revoming Duplicates\n",
    "'''\n",
    "test_url = \"https://minecraft.wiki/w/Blocks\"\n",
    "response = requests.get(test_url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Try to get the canonical URL\n",
    "canonical_tag = soup.find(\"link\", rel=\"canonical\")\n",
    "if canonical_tag and canonical_tag.has_attr(\"href\"):\n",
    "    canonical_url = canonical_tag[\"href\"]\n",
    "else:\n",
    "    canonical_url = response.url\n",
    "\n",
    "print(canonical_url)\n",
    "'''\n",
    "def getRealURL(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Try to get the canonical URL\n",
    "    canonical_tag = soup.find(\"link\", rel=\"canonical\")\n",
    "    if canonical_tag and canonical_tag.has_attr(\"href\"):\n",
    "        canonical_url = canonical_tag[\"href\"]\n",
    "        if '#' in canonical_url:\n",
    "            canonical_url = canonical_url.split('#')[0]\n",
    "    else:\n",
    "        canonical_url = response.url\n",
    "    \n",
    "    return canonical_url\n",
    "print(getRealURL(\"https://minecraft.wiki/w/Blocks\"))\n",
    "print(getRealURL(\"https://minecraft.wiki/w/Tulip#White\"))\n",
    "print(getRealURL(\"https://minecraft.wiki/w/Pink_Banner\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working\n",
    "rp = urllib.robotparser.RobotFileParser()\n",
    "rp.set_url(urljoin(BASE_WIKI_URL, \"/robots.txt\"))\n",
    "# print(urljoin(BASE_URL, \"robots.txt\"))\n",
    "rp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass\n",
    "disallowed_keywords = ['File:', 'Special:', 'Property', 'User:', 'Minecraft_Wiki', 'Help:', 'Minecraft_Legends', 'Minecraft_Dungeons', '=', 'Minecraft_Story_Mode', 'Story_Mode',':','edition']\n",
    "def can_fetch(url):\n",
    "    return not any(keyword in url for keyword in disallowed_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# pass\n",
    "print(can_fetch(\"https://minecraft.wiki/w/Trading\"))\n",
    "print(can_fetch(\"https://minecraft.wiki/w/Special:UserLogin?returnto=Player\"))\n",
    "print(can_fetch(\"https://minecraft.wiki/*?title=Property%3A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Gameplay Trading\n",
    "BASE_URL = \"https://minecraft.wiki/w/Trading\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Trading/\"\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "to_visit = {BASE_URL}\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "with open(SAVE_PATH+\"Trading.txt\", 'w', encoding='utf-8') as file:\n",
    "    main_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    section_title = \"Overview\"\n",
    "    subsection_title = \"\"\n",
    "    content = []\n",
    "\n",
    "    def write_content():\n",
    "        if section_title:\n",
    "            file.write(f\"{section_title}\\n\")\n",
    "        if subsection_title:\n",
    "            file.write(f\"  {subsection_title}\\n\")\n",
    "        if content:\n",
    "            file.write(f\"    {' '.join(content)}\\n\\n\")\n",
    "\n",
    "    def write_table(table):\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer, index=False, header=True)\n",
    "        table_str = csv_buffer.getvalue()\n",
    "        indented_table_str = '    ' + table_str.replace('\\n', '\\n    ')\n",
    "        file.write(\"{}\\n\\n\".format(indented_table_str))\n",
    "    \n",
    "    for element in main_div.find_all(['h2', 'h3', 'p', 'table'], recursive=False):\n",
    "        # print(f'Processing {element.name} tag')\n",
    "        if element.name == 'h2':\n",
    "            # Write previous section\n",
    "            write_content()\n",
    "            content = []\n",
    "            section_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "            subsection_title = \"\"\n",
    "        elif element.name == 'h3':\n",
    "            # Write previous subsection content\n",
    "            write_content()\n",
    "            content = []\n",
    "            section_title = \"\"\n",
    "            subsection_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "        elif element.name == 'p':\n",
    "            content.append(element.text.strip())\n",
    "        elif element.name == 'table':\n",
    "            write_content()\n",
    "            write_table(element)\n",
    "            section_title = \"\"\n",
    "            subsection_title = \"\"\n",
    "            content = []\n",
    "\n",
    "    # Write last section\n",
    "    if content:\n",
    "        write_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Gameplay Brewing\n",
    "BASE_URL = \"https://minecraft.wiki/w/Brewing\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Brewing/\"\n",
    "PAGE_NAME = \"Brewing\"\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Gameplay Enchanting\n",
    "BASE_URL = \"https://minecraft.wiki/w/Enchanting\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Enchanting/\"\n",
    "PAGE_NAME = \"Enchanting\"\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "table = soup.find('table', {'data-description': 'Summary of enchantments'})\n",
    "links_to_crawl = []\n",
    "file_names = []\n",
    "for tr in table.find_all('tr')[1:]:\n",
    "    first_link = tr.find('a')\n",
    "    if first_link and first_link.has_attr('href'):\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, first_link['href'])\n",
    "        links_to_crawl.append(full_url)\n",
    "        file_names.append(first_link['title'])\n",
    "for i in range(len(links_to_crawl)):\n",
    "    crawl_page(links_to_crawl[i], SAVE_PATH, file_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_recipe(url, save_path, pagename):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table_body = soup.find('tbody')\n",
    "    recipes = []\n",
    "    rows = table_body.find_all('tr')[1:]\n",
    "    for tr in rows:\n",
    "        columns = tr.find_all(['th', 'td'])\n",
    "        names = [a.get_text(strip=True) for a in columns[0].find_all('a')]\n",
    "        ingredients = [a.get_text(strip=True) for a in columns[1].find_all('a')]\n",
    "        mcui_input = columns[2].find('span', class_='mcui-input')\n",
    "        if mcui_input:\n",
    "            inv_slots = mcui_input.find_all('span', class_='invslot')\n",
    "            recipe_pattern = []\n",
    "            for slot in inv_slots:\n",
    "                item = slot.find('span', class_='invslot-item')\n",
    "                if item:\n",
    "                    title = item.a.get('title') if item.a else ''\n",
    "                    recipe_pattern.append(title)\n",
    "                else:\n",
    "                    recipe_pattern.append(None)\n",
    "            three_by_three_recipe = [recipe_pattern[i:i+3] for i in range(0, len(recipe_pattern), 3)]\n",
    "        # print(three_by_three_recipe)\n",
    "        description = columns[3].get_text(strip=True) if len(columns) > 3 else \"\"\n",
    "\n",
    "        recipe = {\n",
    "                'names': names,\n",
    "                'ingredients': ingredients,\n",
    "                'sample pattern': three_by_three_recipe,\n",
    "                'description': description\n",
    "            }\n",
    "        recipes.append(recipe)\n",
    "    with open(os.path.join(save_path, f\"{pagename}.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(recipes, f, ensure_ascii=False, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://minecraft.wiki/w/Crafting/Building_blocks', 'https://minecraft.wiki/w/Crafting/Decoration_blocks', 'https://minecraft.wiki/w/Crafting/Redstone', 'https://minecraft.wiki/w/Crafting/Transportation', 'https://minecraft.wiki/w/Crafting/Foodstuffs', 'https://minecraft.wiki/w/Crafting/Tools', 'https://minecraft.wiki/w/Crafting/Combat', 'https://minecraft.wiki/w/Crafting/Brewing', 'https://minecraft.wiki/w/Crafting/Materials', 'https://minecraft.wiki/w/Crafting/Miscellaneous']\n"
     ]
    }
   ],
   "source": [
    "# Crawing for Crafting\n",
    "BASE_URL = \"https://minecraft.wiki/w/Crafting\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Crafting/\"\n",
    "PAGE_NAME = \"Crafating\"\n",
    "\n",
    "# Crawing the main page\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "with open(SAVE_PATH + PAGE_NAME + \".txt\", 'w', encoding='utf-8') as file:\n",
    "    main_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    section_title = \"Overview\"\n",
    "    subsection_title = \"\"\n",
    "    content = []\n",
    "    \n",
    "    for element in main_div.find_all(['h2', 'h3', 'p', 'table'], recursive=False):\n",
    "        # print(f'Processing {element.name} tag')\n",
    "        if element.name == 'h2':\n",
    "            # Write previous section\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            content = []\n",
    "            section_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "            if section_title == \"Complete recipe list\":\n",
    "                break\n",
    "            subsection_title = \"\"\n",
    "        elif element.name == 'h3':\n",
    "            # Write previous subsection + content\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            content = []\n",
    "            section_title = \"\"\n",
    "            subsection_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "        elif element.name == 'p':\n",
    "            content.append(element.text.strip())\n",
    "        elif element.name == 'table':\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            write_table(element, file)\n",
    "            section_title = \"\"\n",
    "            subsection_title = \"\"\n",
    "            content = []\n",
    "\n",
    "    if content:\n",
    "        write_content(file, section_title, subsection_title, content)\n",
    "\n",
    "\n",
    "\n",
    "# Crawing the recipes to json files\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "sections = soup.select('div.load-page')\n",
    "titles = []\n",
    "urls = []\n",
    "for section in sections:\n",
    "    headline = section.find(class_='mw-headline')\n",
    "    if headline:\n",
    "        curr_section = headline.text.strip().replace(' ', \"_\")\n",
    "        titles.append(curr_section)\n",
    "        # print(curr_section)\n",
    "        temp = section.find(class_='hatnote searchaux')\n",
    "        first_link = temp.find('a')\n",
    "        if first_link and first_link.has_attr('href'):\n",
    "            full_url = requests.compat.urljoin(BASE_WIKI_URL, first_link['href'])\n",
    "            urls.append(full_url)\n",
    "            # print(full_url)\n",
    "print(urls)\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    crawl_recipe(urls[i], SAVE_PATH, titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Smelting\n",
    "# Commenting out since Smelting needed manual cleaning\n",
    "'''\n",
    "BASE_URL = \"https://minecraft.wiki/w/Smelting\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Smelting/\"\n",
    "PAGE_NAME = \"Smelting\"\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Smithing\n",
    "BASE_URL = \"https://minecraft.wiki/w/Smithing\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Smithing/\"\n",
    "PAGE_NAME = \"Smithing\"\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Archaeology\n",
    "BASE_URL = \"https://minecraft.wiki/w/Archaeology\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Archaeology/\"\n",
    "PAGE_NAME = \"Archaeology\"\n",
    "\n",
    "def crawl_page(url, save_path, pagename):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    with open(save_path + pagename + \".txt\", 'w', encoding='utf-8') as file:\n",
    "        main_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "        section_title = \"Overview\"\n",
    "        subsection_title = \"\"\n",
    "        content = []\n",
    "        \n",
    "        for element in main_div.find_all(['h2', 'h3', 'ul', 'dl','p', 'table'], recursive=False):\n",
    "            # print(f'Processing {element.name} tag')\n",
    "            if element.name == 'h2':\n",
    "                # Write previous section\n",
    "                write_content(file, section_title, subsection_title, content)\n",
    "                content = []\n",
    "                section_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "                if section_title == \"History\" or section_title == \"Sounds\":\n",
    "                    break\n",
    "                subsection_title = \"\"\n",
    "            elif element.name == 'h3':\n",
    "                # Write previous subsection + content\n",
    "                write_content(file, section_title, subsection_title, content)\n",
    "                content = []\n",
    "                section_title = \"\"\n",
    "                subsection_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "            elif element.name == 'p' or element.name == 'ul' or element.name == 'dl':\n",
    "                content.append(element.text.strip())\n",
    "            elif element.name == 'table':\n",
    "                write_content(file, section_title, subsection_title, content)\n",
    "                write_table(element, file)\n",
    "                section_title = \"\"\n",
    "                subsection_title = \"\"\n",
    "                content = []\n",
    "\n",
    "        if content:\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Redstone circuits\n",
    "BASE_URL = \"https://minecraft.wiki/w/Redstone_circuits\"\n",
    "SAVE_PATH = \"downloaded_pages/Gameplay/Redstone_circuits/\"\n",
    "PAGE_NAME = \"Redstone_circuits\"\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Effect\n",
    "BASE_URL = \"https://minecraft.wiki/w/Effect\"\n",
    "SAVE_PATH = \"downloaded_pages/Effect/\"\n",
    "PAGE_NAME = \"Effect\"\n",
    "crawl_page(BASE_URL, SAVE_PATH, PAGE_NAME)\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "table = soup.find('table', {'data-description': 'Effects'})\n",
    "links_to_crawl = []\n",
    "file_names = []\n",
    "for tr in table.find_all('tr')[1:]:\n",
    "    first_link = tr.find('a')\n",
    "    if first_link and first_link.has_attr('href'):\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, first_link['href'])\n",
    "        links_to_crawl.append(full_url)\n",
    "        file_names.append(first_link['title'])\n",
    "for i in range(len(links_to_crawl)):\n",
    "    crawl_page(links_to_crawl[i], SAVE_PATH, file_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Blocks\n",
    "BASE_URL = \"https://minecraft.wiki/w/Block\"\n",
    "SAVE_PATH = \"downloaded_pages/Block/\"\n",
    "PAGE_NAME = \"Block\"\n",
    "\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "with open(SAVE_PATH + PAGE_NAME + \".txt\", 'w', encoding='utf-8') as file:\n",
    "    main_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    section_title = \"Overview\"\n",
    "    subsection_title = \"\"\n",
    "    content = []\n",
    "    \n",
    "    for element in main_div.find_all(['h2', 'h3', 'p', 'table'], recursive=False):\n",
    "        # print(f'Processing {element.name} tag')\n",
    "        if element.name == 'h2':\n",
    "            # Write previous section\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            content = []\n",
    "            section_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "            if section_title == \"List of blocks\":\n",
    "                break\n",
    "            subsection_title = \"\"\n",
    "        elif element.name == 'h3':\n",
    "            # Write previous subsection + content\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            content = []\n",
    "            section_title = \"\"\n",
    "            subsection_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "        elif element.name == 'p':\n",
    "            content.append(element.text.strip())\n",
    "        elif element.name == 'table':\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            write_table(element, file)\n",
    "            section_title = \"\"\n",
    "            subsection_title = \"\"\n",
    "            content = []\n",
    "\n",
    "    if content:\n",
    "        write_content(file, section_title, subsection_title, content)\n",
    "\n",
    "divs = soup.find_all('div', class_='div-col columns column-width')[:2]\n",
    "names = []\n",
    "to_crawl = set()\n",
    "for li in divs[0].find_all('li'):\n",
    "    a_tags = li.find_all('a')\n",
    "    if len(a_tags) > 1:\n",
    "        name = a_tags[1].get_text(strip=True)\n",
    "        names.append(name)\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, a_tags[1]['href'])\n",
    "        if \"mw-redirect\" in a_tags[1].get('class', []):\n",
    "            full_url = getRealURL(full_url)\n",
    "        to_crawl.add(full_url)\n",
    "with open('Normal_Blocks.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(names, file, indent=4, ensure_ascii=False)\n",
    "SAVE_PATH = \"downloaded_pages/Block/Normal_Blocks/\"\n",
    "for link in to_crawl:\n",
    "    file_name = link.split('/')[-1]\n",
    "    crawl_page(link, SAVE_PATH, file_name)\n",
    "    time.sleep(random.randint(1, 5))\n",
    "\n",
    "names = []\n",
    "to_crawl = set()\n",
    "for li in divs[1].find_all('li'):\n",
    "    a_tags = li.find_all('a')\n",
    "    if len(a_tags) > 1:\n",
    "        name = a_tags[1].get_text(strip=True)\n",
    "        names.append(name)\n",
    "        # realURL = a_tags[1]['href']\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, a_tags[1]['href'])\n",
    "        if \"mw-redirect\" in a_tags[1].get('class', []):\n",
    "            full_url = getRealURL(full_url)\n",
    "        to_crawl.add(full_url)\n",
    "with open('Technical_Blocks.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(names, file, indent=4, ensure_ascii=False)\n",
    "SAVE_PATH = \"downloaded_pages/Block/Technical_Blocks/\"\n",
    "for link in to_crawl:\n",
    "    file_name = link.split('/')[-1]\n",
    "    crawl_page(link, SAVE_PATH, file_name)\n",
    "    time.sleep(random.randint(1, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allay\n",
      "Axolotl\n",
      "Bat\n",
      "Camel\n",
      "Cat\n",
      "Chicken\n",
      "Cod\n",
      "Cow\n",
      "Donkey\n",
      "Fox\n",
      "Allay\n",
      "Axolotl\n",
      "Bat\n",
      "Camel\n",
      "Cat\n",
      "Chicken\n",
      "Cod\n",
      "Cow\n",
      "Donkey\n",
      "Fox\n",
      "Frog\n",
      "Glow Squid\n",
      "Horse\n",
      "Mooshroom\n",
      "Mule\n",
      "Ocelot\n",
      "Parrot\n",
      "Pig\n",
      "Rabbit\n",
      "Salmon\n",
      "Frog\n",
      "Glow Squid\n",
      "Horse\n",
      "Mooshroom\n",
      "Mule\n",
      "Ocelot\n",
      "Parrot\n",
      "Pig\n",
      "Rabbit\n",
      "Salmon\n",
      "Sheep\n",
      "Sniffer\n",
      "Snow Golem\n",
      "Squid\n",
      "Strider\n",
      "Tadpole\n",
      "Tropical Fish\n",
      "Turtle\n",
      "Villager\n",
      "Wandering Trader\n",
      "Sheep\n",
      "Sniffer\n",
      "Snow Golem\n",
      "Squid\n",
      "Strider\n",
      "Tadpole\n",
      "Tropical Fish\n",
      "Turtle\n",
      "Villager\n",
      "Wandering Trader\n"
     ]
    }
   ],
   "source": [
    "# Crawing for Mob\n",
    "BASE_URL = \"https://minecraft.wiki/w/Mob\"\n",
    "SAVE_PATH = \"downloaded_pages/Mob/\"\n",
    "PAGE_NAME = \"Mob\"\n",
    "\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "with open(SAVE_PATH + PAGE_NAME + \".txt\", 'w', encoding='utf-8') as file:\n",
    "    main_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    section_title = \"Overview\"\n",
    "    subsection_title = \"\"\n",
    "    content = []\n",
    "    \n",
    "    for element in main_div.find_all(['h2', 'h3', 'p', 'table'], recursive=False):\n",
    "        # print(f'Processing {element.name} tag')\n",
    "        if element.name == 'h2':\n",
    "            # Write previous section\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            content = []\n",
    "            section_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "            if section_title == \"Upcoming mobs\":\n",
    "                break\n",
    "            subsection_title = \"\"\n",
    "        elif element.name == 'h3':  \n",
    "            # Write previous subsection + content\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            content = []\n",
    "            section_title = \"\"\n",
    "            subsection_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "        elif element.name == 'p':\n",
    "            content.append(element.text.strip())\n",
    "        elif element.name == 'table':\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            write_table(element, file)\n",
    "            section_title = \"\"\n",
    "            subsection_title = \"\"\n",
    "            content = []\n",
    "\n",
    "    if content:\n",
    "        write_content(file, section_title, subsection_title, content)\n",
    "\n",
    "\n",
    "tables = soup.find_all('table')[:10]\n",
    "to_crawl = []\n",
    "name = []\n",
    "for x in range(3):\n",
    "    a_tags = tables[x].find_all('a')\n",
    "    for a in a_tags:\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, a['href'])\n",
    "        to_crawl.append(full_url)\n",
    "        name.append(a['title'])\n",
    "SAVE_PATH = \"downloaded_pages/Mob/Passive_Mobs/\"\n",
    "for i in range(len(to_crawl)):\n",
    "    crawl_page(to_crawl[i], SAVE_PATH, name[i])\n",
    "\n",
    "to_crawl = []\n",
    "name = []\n",
    "a_tags = tables[3].find_all('a')\n",
    "for a in a_tags:\n",
    "    full_url = requests.compat.urljoin(BASE_WIKI_URL, a['href'])\n",
    "    to_crawl.append(full_url)\n",
    "    name.append(a['title'])\n",
    "SAVE_PATH = \"downloaded_pages/Mob/Defensive_Mobs/\"\n",
    "for i in range(len(to_crawl)):\n",
    "    crawl_page(to_crawl[i], SAVE_PATH, name[i])\n",
    "\n",
    "\n",
    "to_crawl = []\n",
    "name = []\n",
    "for x in range(4,6):\n",
    "    a_tags = tables[x].find_all('a')\n",
    "    for a in a_tags:\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, a['href'])\n",
    "        to_crawl.append(full_url)\n",
    "        name.append(a['title'])\n",
    "SAVE_PATH = \"downloaded_pages/Mob/Neutral_Mobs/\"\n",
    "for i in range(len(to_crawl)):\n",
    "    crawl_page(to_crawl[i], SAVE_PATH, name[i])\n",
    "\n",
    "to_crawl = []\n",
    "name = []\n",
    "for x in range(6,9):\n",
    "    a_tags = tables[x].find_all('a')\n",
    "    for a in a_tags:\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, a['href'])\n",
    "        to_crawl.append(full_url)\n",
    "        name.append(a['title'])\n",
    "SAVE_PATH = \"downloaded_pages/Mob/Hostile_Mobs/\"\n",
    "for i in range(len(to_crawl)):\n",
    "    crawl_page(to_crawl[i], SAVE_PATH, name[i])\n",
    "\n",
    "to_crawl = []\n",
    "name = []\n",
    "a_tags = tables[9].find_all('a')\n",
    "for a in a_tags:\n",
    "    full_url = requests.compat.urljoin(BASE_WIKI_URL, a['href'])\n",
    "    to_crawl.append(full_url)\n",
    "    name.append(a['title'])\n",
    "SAVE_PATH = \"downloaded_pages/Mob/Boss_Mobs/\"\n",
    "for i in range(len(to_crawl)):\n",
    "    crawl_page(to_crawl[i], SAVE_PATH, name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Items\n",
    "BASE_URL = \"https://minecraft.wiki/w/Items\"\n",
    "SAVE_PATH = \"downloaded_pages/Items/\"\n",
    "PAGE_NAME = \"Items\"\n",
    "\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "with open(SAVE_PATH + PAGE_NAME + \".txt\", 'w', encoding='utf-8') as file:\n",
    "    main_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    section_title = \"Overview\"\n",
    "    subsection_title = \"\"\n",
    "    content = []\n",
    "    \n",
    "    for element in main_div.find_all(['h2', 'h3', 'p', 'table'], recursive=False):\n",
    "        # print(f'Processing {element.name} tag')\n",
    "        if element.name == 'h2':\n",
    "            # Write previous section\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            content = []\n",
    "            section_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "            if section_title == \"List of blocks\":\n",
    "                break\n",
    "            subsection_title = \"\"\n",
    "        elif element.name == 'h3':\n",
    "            # Write previous subsection + content\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            content = []\n",
    "            section_title = \"\"\n",
    "            subsection_title = element.text.strip().replace('[edit | edit source]', '')\n",
    "        elif element.name == 'p':\n",
    "            content.append(element.text.strip())\n",
    "        elif element.name == 'table':\n",
    "            write_content(file, section_title, subsection_title, content)\n",
    "            write_table(element, file)\n",
    "            section_title = \"\"\n",
    "            subsection_title = \"\"\n",
    "            content = []\n",
    "\n",
    "    if content:\n",
    "        write_content(file, section_title, subsection_title, content)\n",
    "\n",
    "divs = soup.find_all('div', class_='div-col columns column-width')[:3]\n",
    "names = []\n",
    "to_crawl = []\n",
    "for li in divs[0].find_all('li'):\n",
    "    a_tags = li.find_all('a')\n",
    "    if len(a_tags) > 1:\n",
    "        name = a_tags[1].get_text(strip=True)\n",
    "        names.append(name)\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, a_tags[1]['href'])\n",
    "        if \"mw-redirect\" in a_tags[1].get('class', []):\n",
    "            full_url = getRealURL(full_url)\n",
    "        to_crawl.append(full_url)\n",
    "with open('Items_for_Creation.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(names, file, indent=4, ensure_ascii=False)\n",
    "SAVE_PATH = \"downloaded_pages/Items/Items_for_Creation/\"\n",
    "for link in to_crawl:\n",
    "    file_name = link.split('/')[-1]\n",
    "    crawl_page(link, SAVE_PATH, file_name)\n",
    "    time.sleep(random.randint(1, 5))\n",
    "\n",
    "names = []\n",
    "to_crawl = []\n",
    "for li in divs[1].find_all('li'):\n",
    "    a_tags = li.find_all('a')\n",
    "    if len(a_tags) > 1:\n",
    "        name = a_tags[1].get_text(strip=True)\n",
    "        names.append(name)\n",
    "        # realURL = a_tags[1]['href']\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, a_tags[1]['href'])\n",
    "        if \"mw-redirect\" in a_tags[1].get('class', []):\n",
    "            full_url = getRealURL(full_url)\n",
    "        to_crawl.append(full_url)\n",
    "with open('Items_for_Use.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(names, file, indent=4, ensure_ascii=False)\n",
    "SAVE_PATH = \"downloaded_pages/Items/Items_for_Use/\"\n",
    "for link in to_crawl:\n",
    "    file_name = link.split('/')[-1]\n",
    "    crawl_page(link, SAVE_PATH, file_name)\n",
    "    time.sleep(random.randint(1, 5))\n",
    "\n",
    "\n",
    "names = []\n",
    "to_crawl = []\n",
    "for li in divs[2].find_all('li'):\n",
    "    a_tags = li.find_all('a')\n",
    "    if len(a_tags) > 1:\n",
    "        name = a_tags[1].get_text(strip=True)\n",
    "        names.append(name)\n",
    "        # realURL = a_tags[1]['href']\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, a_tags[1]['href'])\n",
    "        if \"mw-redirect\" in a_tags[1].get('class', []):\n",
    "            full_url = getRealURL(full_url)\n",
    "        to_crawl.append(full_url)\n",
    "with open('Items_for_Indirect_Use.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(names, file, indent=4, ensure_ascii=False)\n",
    "SAVE_PATH = \"downloaded_pages/Items/Items_for_Indirect_Use/\"\n",
    "for link in to_crawl:\n",
    "    file_name = link.split('/')[-1]\n",
    "    crawl_page(link, SAVE_PATH, file_name)\n",
    "    time.sleep(random.randint(1, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawing for Biome\n",
    "BASE_URL = \"https://minecraft.wiki/w/Biome\"\n",
    "SAVE_PATH = \"downloaded_pages/Biome/\"\n",
    "PAGE_NAME = \"Biome\"\n",
    "\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "tables = soup.find_all('table', class_='wikitable collapsible')\n",
    "for table in tables:\n",
    "    curr = table['data-description']\n",
    "    path = SAVE_PATH + curr + '/'\n",
    "    bs =  table.findAll('b')\n",
    "    for b in bs:\n",
    "        a = b.find('a')\n",
    "        if a:\n",
    "            file_name = a.get_text(strip=True)\n",
    "            full_url = requests.compat.urljoin(BASE_WIKI_URL, a['href'])\n",
    "            crawl_page(full_url, path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: \"downloaded_pages/Tutorials/Introductory/The_first_day/beginner's_guide.txt\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bb762\\Desktop\\temp\\Data_Init.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m fname \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X32sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m full_url \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39murljoin(BASE_WIKI_URL, a[\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m crawl_page(full_url, SAVE_PATH, fname)\n",
      "\u001b[1;32mc:\\Users\\bb762\\Desktop\\temp\\Data_Init.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X32sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m response\u001b[39m.\u001b[39mraise_for_status()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X32sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X32sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(save_path \u001b[39m+\u001b[39;49m pagename \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X32sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     main_div \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, {\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mmw-parser-output\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/bb762/Desktop/temp/Data_Init.ipynb#X32sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m     section_title \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOverview\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"downloaded_pages/Tutorials/Introductory/The_first_day/beginner's_guide.txt\""
     ]
    }
   ],
   "source": [
    "# Crawing for Tutorials\n",
    "BASE_URL = \"https://minecraft.wiki/w/Tutorials\"\n",
    "SAVE_PATH = \"downloaded_pages/Tutorials/\"\n",
    "PAGE_NAME = \"Tutorials\"\n",
    "\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "response = requests.get(BASE_URL)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "table = soup.find('table')\n",
    "trs = table.find_all('tr')[1:]\n",
    "for tr in trs:\n",
    "    title = tr.find('th').text.strip()\n",
    "    a_blocks = tr.find_all('a')\n",
    "    SAVE_PATH = \"downloaded_pages/Tutorials/\" + title + \"/\"\n",
    "    for a in a_blocks:\n",
    "        fname = a.text.replace(' ', '_')\n",
    "        fname = fname.replace('/', '_')\n",
    "        full_url = requests.compat.urljoin(BASE_WIKI_URL, a['href'])\n",
    "        crawl_page(full_url, SAVE_PATH, fname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling and saving finished!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "while to_visit:\n",
    "    current_url = to_visit.pop()\n",
    "\n",
    "    if current_url in visited_urls or not can_fetch(current_url):\n",
    "        continue\n",
    "\n",
    "    response = requests.get(current_url)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    canonical_tag = soup.find(\"link\", rel=\"canonical\")\n",
    "\n",
    "    if canonical_tag[\"href\"]:\n",
    "        # Check Redirect\n",
    "        if not canonical_tag[\"href\"] == current_url:\n",
    "            visited_urls.add(current_url)\n",
    "            current_url = canonical_tag[\"href\"]\n",
    "            if current_url in visited_urls:\n",
    "                continue\n",
    "\n",
    "    \n",
    "    page_name = urlparse(current_url).path.split('/')[-1] or 'index'\n",
    "    page_name = page_name.replace(':', '_').replace('%', '_').replace('?', '_') + \".html\"\n",
    "    with open(os.path.join(SAVE_PATH, page_name), 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    # Extract links\n",
    "    for link in soup.find_all('a', href=True):\n",
    "            full_url = link['href'] if 'http' in link['href'] else BASE_URL + link['href']\n",
    "            full_url = sanitize_url(full_url)\n",
    "            if not can_fetch(full_url):\n",
    "                continue\n",
    "            if full_url not in visited_urls and BASE_WIKI_URL in full_url:\n",
    "                to_visit.add(full_url)\n",
    "\n",
    "    visited_urls.add(current_url)\n",
    "\n",
    "    time.sleep(random.uniform(1, 4))\n",
    "\n",
    "print(\"Crawling and saving finished!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n",
      "3466\n"
     ]
    }
   ],
   "source": [
    "print('https://minecraft.wiki/w/Acacia_Button' in to_visit)\n",
    "print(len(to_visit))\n",
    "print(len(visited_urls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
